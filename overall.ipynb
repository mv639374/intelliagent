{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9e4484",
   "metadata": {},
   "source": [
    "# IntelliAgent\n",
    "\n",
    "_Enterprise-Grade Agentic AI Platform - More details coming soon._\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# **Phase-0** (Foundation Setup)\n",
    "\n",
    "\n",
    "## Step 0.1 | Initialize repository \n",
    "* Makefile\n",
    "* .gitignore\n",
    "* .env.example && backend/.env.example && frontend/.env.example\n",
    "* README.md\n",
    "* License\n",
    "\n",
    "```bash\n",
    "make help\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "## Step 0.2 | Backend Python Environment\n",
    "* backend/pyproject.toml\n",
    "\n",
    "```bash\n",
    "# Run all these from backend directory\n",
    "\n",
    "pyenv install 3.11.9  # install\n",
    "pyenv local 3.11.9    # make it local\n",
    "python --version  # check version\n",
    "pyenv install --list | grep 3.11  # check available python versions\n",
    "\n",
    "# setup virtual enviroment\n",
    "uv venv\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Install dependencies\n",
    "uv pip install -e \".[dev]\" \n",
    "# or\n",
    "make install-backend\n",
    "# or\n",
    "pip install -e .\n",
    "\n",
    "# then run\n",
    "uv run ruff check .\n",
    "uv run mypy app\n",
    "\n",
    "# verify it prints OK\n",
    "uv run python -c \"import fastapi, langchain, langgraph; print('OK')\"\n",
    "```\n",
    "\n",
    "## Step 0.3 | Frontend Next.js Environment\n",
    "`Will come back to it later`\n",
    "```bash\n",
    "npx create-next-app@latest frontend --typescript --tailwind --app --eslint\n",
    "npm install \n",
    "# or\n",
    "make install-frontend\n",
    "npm run dev\n",
    "npm run build\n",
    "npm run lint\n",
    "```\n",
    "\n",
    "## Step 0.4 | Docker Compose for local development\n",
    "* docker-compose.yml\n",
    "* docker-compose.dev.yml\n",
    "* docker-compose.prod.yml\n",
    "* deploy/.dockerignore\n",
    "* deploy/Dockerfile.api\n",
    "* deploy/Dockerfile.web\n",
    "* deploy/Dockerfile.worker\n",
    "\n",
    "### Services:\n",
    "* postgres:16-alpine — port 5432, volume postgres_data\n",
    "* redis:7-alpine — port 6379, volume redis_data\n",
    "* elasticsearch:8.11.0 or opensearchproject/opensearch:2 — port 9200, single-node, volume es_data\n",
    "* qdrant/qdrant or chromadb/chroma — port 6333/8000, volume vector_data\n",
    "* api service — build ./backend, depends on db/redis/es, port 8000, env file, hot-reload volume mounts\n",
    "* web service — build ./frontend, depends on api, port 3000, env file, hot-reload\n",
    "* worker service — build ./workers, depends on redis, env file\n",
    "\n",
    "```bash\n",
    "# Build & Run the services\n",
    "docker-compose -f docker-compose.dev.yml up -d --build\n",
    "# or\n",
    "make run-dev\n",
    "# Start DBs (docker compose -f docker-compose.dev.yml up -d postgres redis elasticsearch)\n",
    "\n",
    "# shows all containers healthy\n",
    "docker compose -f docker-compose.dev.yml ps\n",
    "\n",
    "# Connect to postgres: \n",
    "psql postgres://user:pass@localhost:5432/dbname\n",
    "\n",
    "# Connect to redis (returns PONG): \n",
    "redis-cli -h localhost ping \n",
    "\n",
    "# Elasticsearch (returns cluster info): \n",
    "curl http://localhost:9200\n",
    "\n",
    "# Set DOCKERHUB_USERNAME and DOCKERHUB_TOKEN as environment variables in github\n",
    "```\n",
    "\n",
    "### Some useful docker commands\n",
    "```bash\n",
    "# Install a library inside container\n",
    "docker exec -it intelliagent-backend python3.11 -m pip install pandas\n",
    "\n",
    "# Run a file inside docker\n",
    "docker exec -it intelliagent-backend sh -c \"cd /app && PYTHONPATH=/app python abc/xyz.py\"\n",
    "\n",
    "# Run Tests\n",
    "docker compose -f docker-compose.dev.yml exec backend python -m pytest tests/ -v\n",
    "# or a particular test\n",
    "docker compose -f docker-compose.dev.yml exec backend python -m pytest tests/test.py\n",
    "\n",
    "### Remove unused images\n",
    "docker image prune -a\n",
    "\n",
    "### Remove build cache (if needed)\n",
    "docker builder prune -a\n",
    "\n",
    "### Remove everything and start fresh\n",
    "docker system prune -a --volumes\n",
    "```\n",
    "\n",
    "## Step 0.5 | CI/CD pipeline skeleton\n",
    "* .github/workflows/ ci.yml cd.yml lint.yml test.yml\n",
    "* .pre-commit-config.yaml\n",
    "* scripts/healthcheck.sh\n",
    "\n",
    "### CI jobs\n",
    "* Lint backend (ruff, mypy)\n",
    "* Lint frontend (eslint, tsc)\n",
    "* Run backend unit tests (pytest)\n",
    "* Run frontend unit tests (jest/vitest)\n",
    "* Build Docker images (api, web, worker)\n",
    "\n",
    "```bash\n",
    "# Install the tool (from root project folder): \n",
    "pip install pre-commit\n",
    "\n",
    "# Set up the git hooks: \n",
    "pre-commit install\n",
    "\n",
    "# Then every time before git commit\n",
    "pre-commit run --all-files\n",
    "\n",
    "# Exits 0 when services are up\n",
    "chmod +x scripts/healthcheck.sh\n",
    "./scripts/healthcheck.sh\n",
    "```\n",
    "\n",
    "## Step 0.6 | Documentation Scaffolding\n",
    "* docs/architecture.md — High-level architecture, component diagram\n",
    "* docs/development-guide.md — Setup, workflows, standards\n",
    "* docs/rag-design.md — RAG pipeline design (to be detailed in Phase 1)\n",
    "* docs/mcp-integration.md — MCP design (to be detailed in Phase 2)\n",
    "* docs/security-policies.md — Security model (to be detailed in Phase 3)\n",
    "* docs/ops-runbook.md — Operations guide (to be detailed in Phase 4)\n",
    "* docs/api-specification.md — Placeholder for OpenAPI spec\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# **Phase-1** (Core RAG & Data Pipelines)\n",
    "\n",
    "* Now everything will be in the backend directory\n",
    "\n",
    "* Document Ingestion\n",
    "* OCR\n",
    "* Chunking\n",
    "* Embedding\n",
    "* Indexing\n",
    "* Hybrid Retrieval (keyword: elasticsearch; vector: qdrant)\n",
    "\n",
    "## Step 1.1 | Database Schema & Models\n",
    "\n",
    "* alembic.ini (update after initialiing alembic)\n",
    "* alembic/env.py (update after initialiing alembic)\n",
    "* app/db/session.py    (This file creates the async session factory that our application will use to interact with the database.)\n",
    "* app/db/base.py\n",
    "* app/db/base_class.py\n",
    "* app/models/user.py\n",
    "* app/models/project.py\n",
    "* app/models/document.py\n",
    "* app/models/chunk.py\n",
    "* app/models/conversation.py\n",
    "* app/models/message.py\n",
    "* app/models/audit.py\n",
    "* scripts/manual_db_test.py\n",
    "\n",
    "\n",
    "```bash\n",
    "# Run from ./backend/ directory\n",
    "alembic init alembic\n",
    "\n",
    "# Generate the migration file and Apply the migration\n",
    "docker exec -it intelliagent-backend bash -c \"cd /app && alembic revision --autogenerate -m 'initial_schema' && alembic upgrade head\"\n",
    "\n",
    "# Restart backend\n",
    "docker compose -f docker-compose.dev.yml restart backend\n",
    "#or only build backend again\n",
    "docker compose -f docker-compose.dev.yml up --build -d backend\n",
    "\n",
    "# Check all tables in db\n",
    "docker exec -it intelliagent-db psql -U user -d intelliagent_db\n",
    "# then '\\dt' for tables and '\\dt documents' for particular table\n",
    "\n",
    "# It should return OK\n",
    "uv run python -c \"from app.models import User, Document, Chunk; print('OK')\"\n",
    "\n",
    "# Or if you want to drop everything in the database\n",
    "docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"DROP SCHEMA public CASCADE; CREATE SCHEMA public;\"\n",
    "rm -rf backend/alembic/versions/*.py\n",
    "\n",
    "# Delete all documents & chunks\n",
    "docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"DELETE FROM chunks; DELETE FROM documents;\"\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------Some more DB commands---------------------------------------\n",
    "\n",
    "# 1. Check chunks table for PII redaction and metadata:\n",
    "\n",
    "`docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"SELECT text, chunk_metadata FROM chunks WHERE document_id = (SELECT id FROM documents WHERE filename = 'pii_sample.pdf');\"`\n",
    "\n",
    "# 2. Check chunks table for tables:\n",
    "\n",
    "`docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"SELECT text FROM chunks WHERE document_id = (SELECT id FROM documents WHERE filename = 'table_sample.pdf') AND text LIKE 'Table:%';\"`\n",
    "\n",
    "# 3. Check documents table for status INDEXED:\n",
    "\n",
    "`docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"SELECT filename, status FROM documents;\"`\n",
    "\n",
    "\n",
    "# Get whole list of users\n",
    "`docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"SELECT id, username, email, role, created_at FROM users;\"`\n",
    "# Delete all of them\n",
    "`docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"TRUNCATE TABLE users CASCADE;\"`\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Step 1.2 | Document Ingestion Pipeline\n",
    "\n",
    "* spaCy model needed for PII redaction\n",
    "\n",
    "* app/rag/ingest/base_connector.py      (This is the abstract base class that all our connectors will inherit from.)\n",
    "* app/rag/ingest/file_connector.py      (Handles local file uploads from the API.)\n",
    "* app/rag/ingest/s3_connector.py        (A basic connector for fetching documents from an S3 bucket.)\n",
    "* app/rag/ingest/drive_connector.py     (A placeholder for the Google Drive connector.)\n",
    "* app/rag/ingest/preprocessor.py        (Cleans and normalizes raw text content.)\n",
    "* app/rag/ingest/pii_redactor.py        (Detects and redacts Personally Identifiable Information (PII).)\n",
    "* app/rag/ingest/metadata_extractor.py  (Extracts metadata like author and title from files.)\n",
    "* workers/tasks/ingest_tasks.py         (This is the Celery task that orchestrates the entire ingestion process asynchronously.)\n",
    "* app/api/v1/documents.py               (The FastAPI endpoint for uploading documents.)\n",
    "\n",
    "* workers/celery_app.py                 (This file defines the Celery application instance, configuring it to use Redis as the broker.)\n",
    "* app/settings.py                       (This file uses Pydantic to manage application settings and load them from environment variables.)\n",
    "* app/main.py                           (This is the main entry point for the FastAPI application.)\n",
    "* app/dependencies.py                   (A place for common, project-wide dependencies. For now, it will just re-export the database session getter.)\n",
    "* app/exceptions.py                     (A place for custom exception handlers. (Empty for now).)\n",
    "* app/middleware.py                     (A place for custom middleware. (Empty for now).)\n",
    "* app/api/v1/router.py                  (This router combines all the individual API endpoints for the v1 version of our API.)\n",
    "* app/api/deps.py                       (This file will contain dependencies specific to the API, like user authentication. For now, we will create stubs.)\n",
    "* tests/rag/ingest/test_file_connector.py\n",
    "\n",
    "```bash\n",
    "# Start the celery worker (run from root, then upload a pdf at POST upload endpoint):\n",
    "docker compose -f docker-compose.dev.yml exec backend celery -A workers.celery_app worker --loglevel=info\n",
    "# or \n",
    "docker compose -f docker-compose.dev.yml exec backend celery -A workers.celery_app worker -l info --pool=solo\n",
    "\n",
    "# then check the database table\n",
    "docker exec -it intelliagent-db psql -U user -d intelliagent_db -c \"SELECT filename, status FROM documents;\"\n",
    "```\n",
    "\n",
    "\n",
    "## Step 1.3 | OCR Integration\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# **Phase 2** (Agentic Orchestration)\n",
    "\n",
    "* Build LangGraph multi-agent graphs with task-specific agents, tool calling, memory, interrupts, and streaming\n",
    "\n",
    "## Step-2.1 | LangGraph Foundation\n",
    "\n",
    "* app/models/graph_checkpoint.py         (defines the graph_checkpoints table schema using SQLAlchemy.)\n",
    "* app/agents/config.py                   (This file centralizes configuration for our agents, allowing us to easily switch between LLM providers based on your priority .)\n",
    "* app/agents/state.py                    (This defines the AgentState, the shared memory structure that is passed between all nodes in our graph.)\n",
    "* app/agents/checkpointer.py             (This class implements LangGraph's persistence layer, saving and loading the state from our Postgres database.)\n",
    "* app/agents/interrupts.py               (This file contains the logic for pausing the graph to wait for human approval.)\n",
    "* app/agents/graph_factory.py            (This is a placeholder for the graph builder. For this step, it will create a simple two-node graph to pass the checkpoint tests.)\n",
    "* app/db/base.py                         (update: import this new model so alembic can discover it)\n",
    "* scripts/agent_foundation_test.py       (to validate the foundation.)\n",
    "* tests/agents/test_graph_factory.py     (building the simple graph and asserting that the expected placeholder nodes (\"start\" and \"end\") exist in the compiled graph structure.)\n",
    "\n",
    "## Step-2.2 | Agent Nodes Implementation\n",
    "\n",
    "* app/agents/nodes/base_node.py     (This abstract base class provides a consistent structure for all our agent nodes.)\n",
    "* app/agents/nodes/planner.py       (This node is responsible for breaking down the user's query into a sequence of actionable steps. )\n",
    "* app/agents/nodes/retriever.py     (This node uses the powerful hybrid RAG retriever we built in Phase 1 to fetch relevant context from our knowledge base.)\n",
    "* app/agents/nodes/solver.py        (The Solver's job is to synthesize the retrieved context into a coherent answer, complete with citations.)\n",
    "* app/agents/nodes/verifier.py      (The Verifier acts as a quality control gate, ensuring the generated answer is grounded in the provided context by checking for citations.)\n",
    "* app/agents/nodes/tool_node.py     (This node is a placeholder for executing tool calls. It simulates finding a tool call and appending a mock result to the context.)\n",
    "\n",
    "* tests/agents/nodes/test_planner.py\n",
    "* tests/agents/nodes/test_retriever.py\n",
    "* tests/agents/nodes/test_solver.py\n",
    "* tests/agents/nodes/test_verifier.py\n",
    "* tests/agents/nodes/test_tool_node.py\n",
    "* scripts/node_checkpoint_test.py\n",
    "\n",
    "## Step-2.3 | Conditional Edges & Routing\n",
    "\n",
    "* app/agents/edges/routing_logic.py\n",
    "* app/agents/edges/conditional_edges.py\n",
    "* tests/agents/test_agent_execution.py\n",
    "* app/agents/state.py                     (update)\n",
    "* app/agents/graph_factory.py             (update)\n",
    "\n",
    "## Phase 2.4 | Memory & Context Management\n",
    "\n",
    "* app/agents/state.py   (update)\n",
    "* app/agents/memory/postgres_store.py\n",
    "* app/agents/memory/redis_store.py\n",
    "* app/agents/memory/memory_management.py\n",
    "* tests/agents/memory/test_memory_manager.py\n",
    "* scripts/memory_checkpoint_test.py\n",
    "\n",
    "# Step 2.5 | Streaming & Real-time Updates\n",
    "\n",
    "* app/agents/graph_factory.py                    (update file: We need to add a new node to the beginning of our graph that loads the conversation history into the state. This makes the agent aware of the conversation's context before it starts planning.)\n",
    "* app/api/v1/chat.py                             (This file is updated with a new /stream endpoint that uses FastAPI's StreamingResponse to stream events directly from the LangGraph astream_events method.)\n",
    "* frontend/src/app/page.tsx                      (display the stream)\n",
    "* frontend/src/components/chat/ChatStream.tsx    (This is the main chat component that handles user input, initiates the stream, and renders the conversation.)\n",
    "* tests/api/v1/test_streaming.py\n",
    "\n",
    "# Step 2.6"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
